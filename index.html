<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="PAVAS: Physics-Aware Video-to-Audio Synthesis.">
  <meta name="keywords" content="Audio-visual learning, LLM, Hallucination">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>PAVAS: Physics-Aware Video-to-Audio Synthesis</title>


  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
<!--  <div class="navbar-menu">-->
<!--    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">-->
<!--      <a class="navbar-item" href="https://keunhong.com">-->
<!--      <span class="icon">-->
<!--          <i class="fas fa-home"></i>-->
<!--      </span>-->
<!--      </a>-->
<!--    </div>-->

<!--  </div>-->
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">

        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">PAVAS: Physics-Aware Video-to-Audio Synthesis</h1>


          <div class="is-size-4 publication-authors">
            <span class="author-block"><b>arXiv 2025</b></span>
          </div>


          <div class="is-size-7 publication-authors">
            <span class="author-block"></span>
          </div>



          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://hyunbin70.github.io/">Oh Hyun-Bin</a><sup>1</sup>,</span>
            <span class="author-block">
                <a href="https://scholar.google.com/citations?user=ahqdEYUAAAAJ&hl=ja">Yuhta Takida</a><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=wd00kkMAAAAJ&hl=ja">Toshimitsu Uesaka</a><sup>2</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://ami.kaist.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a><sup>4</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://www.yukimitsufuji.com/">Yuki Mitsufuji</a><sup>2,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>POSTECH,&nbsp</span>
            <span class="author-block"><sup>2</sup>Sony AI&nbsp</span>
            <span class="author-block"><sup>3</sup>Sony Group Corporation&nbsp</span>
            <span class="author-block"><sup>4</sup>KAIST&nbsp</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link.
              <span class="link-block">
                                <a href="https://openreview.net/pdf?id=jTEKTdI3K9"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span> -->
              <span class="link-block">
                <a href="https://physics-aware-video-to-audio-synthesis.github.io/" class="external-link button is-normal is-rounded is-dark">
<!--                <a class="external-link button is-normal is-rounded is-dark">-->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=zYnUyPHPEn8&t=3s"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
<!--                <a href="https://github.com/google/nerfies"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
                <a href="https://physics-aware-video-to-audio-synthesis.github.io/"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (coming soon)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
<!--              <span class="link-block">-->
<!--                <a href="https://github.com/google/nerfies/releases/tag/0.1"-->
<!--                   class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                      <i class="far fa-images"></i>-->
<!--                  </span>-->
<!--                  <span>Data</span>-->
<!--                  </a>-->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="section">
<div class="content has-text-justified">
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 preload
                 playsinline
                 width="50%">
            <source src="./static/video/v3.mp4"
                    type="video/mp4">
          </video>
        </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Updated Section Title -->
    <div class="columns is-centered has-text-centered">
      <h3 class="title is-4">Demo Video</h3>
    </div>
    
    <!-- Figures Container -->
    <div class="container">
      

      <!-- Demo Video -->
      <div class="columns is-centered">
        <div class="column is-full has-text-centered">
          <video controls style="width: 100%; max-width: 80%; margin: 0 auto;">
            <source src="./static/images/PAVAS_demo_video.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
      
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="columns is-centered has-text-centered">
  <div class="container is-max-desktop">
    <div class="hero-body">

      <img src="./static/images/teaser_pavas.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 70%;"
      />

      <h2 class="subtitle">
        <!-- We introduce <span class="dnerf">AVHBench</span>, the first comprehensive benchmark specifically designed to evaluate the perception and comprehension capabilities of audio-visual LLMs. -->
        We introduce Physics-Aware Video-to-Audio Synthesis (PAVAS). [Top] Current V2A models often generate physically inconsistent audio. [Bottom] We estimate physics values (object-level mass and velocity) from an input video using Physics Parameter Estimator, which are explicitly integrated into a latent diffusion-based model using Phy-Adapter to generate a physically plausible audio.
      </h2>
    </div>
  </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Recent advances in Video-to-Audio (V2A) generation have achieved impressive perceptual quality and temporal synchronization, yet most models remain appearance-driven, capturing visual-acoustic correlations without considering the physical factors that shape real-world sounds.
          We present Physics-Aware Video-to-Audio Synthesis (PAVAS), a method that incorporates physical reasoning into a latent diffusion-based V2A generation through the Physics-Driven Audio Adapter (Phy-Adapter).
          The adapter receives object-level physical parameters estimated by the Physical Parameter Estimator (PPE), which uses a Vision-Language Model (VLM) to infer the moving-object mass and a segmentation-based dynamic 3D reconstruction module to recover its motion trajectory for velocity computation.
          These physical cues enable the model to synthesize sounds that reflect underlying physical factors.
          To assess physical realism, we curate VGG-Impact, a benchmark focusing on object-object interactions, and introduce Audio-Physics Correlation Coefficient (APCC), an evaluation metric that measures consistency between physical and auditory attributes.
          Comprehensive experiments show that PAVAS produces physically plausible and perceptually coherent audio, outperforming existing V2A models in both quantitative and qualitative evaluations.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!Need to add an presentation video this part!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! -->
<!--    <div class="columns is-centered has-text-centered">-->
<!--      <div class="column is-four-fifths">-->
<!--        <h2 class="title is-3">Video</h2>-->
<!--        <div class="publication-video">-->
<!--          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"-->
<!--                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-4">PAVAS Pipeline</h2>
          <div class="column is-full-width has-text-centered">
            <figure class="image" style="width: 100%; max-width: 80%; margin: 0 auto;">
              <img src="./static/images/method_pavas.png" class="interpolation-image" alt="Interpolate start reference image."/>
            </figure>
            <br><br>
            <div class="columns is-centered has-text-centered">
              <div class="column is-full-width">
                <div class="content has-text-justified" style="max-width: 80%; margin: 0 auto;">
                  <p>
                    Given an input video, the Physics Parameter Estimator (PPE) extracts object-level mass and velocity.
                    These physics cues are encoded by the Physics-Driven Audio Adapter (Phy-Adapter) and injected into the latent diffusion model alongside multimodal conditions.
                  </p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>
  


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Updated Section Title -->
      <div class="columns is-centered has-text-centered">
        <h3 class="title is-4">Qualitative Comparison on VGGSound test split</h3>
      </div>
      
      <!-- Figures Container -->
      <div class="container">
        
        <!-- Figure 1 -->
        <div class="columns is-centered">
          <div class="column is-full has-text-centered">
            <figure class="image" style="width: 100%; max-width: 80%; margin: 0 auto;">
              <img src="./static/images/qual_1_pavas.png" alt="Sample 1">
            </figure>
            <p><strong>Samples 1&2</strong></p>
          </div>
        </div>
        
        <!-- Figure 2 -->
        <div class="columns is-centered">
          <div class="column is-full has-text-centered">
            <figure class="image" style="width: 100%; max-width: 80%; margin: 0 auto;">
              <img src="./static/images/qual_2_pavas.png" alt="Sample 2">
            </figure>
            <p><strong>Samples 3&4</strong></p>
          </div>
        </div>

        <!-- Figure 2 -->
        <div class="columns is-centered">
          <div class="column is-full has-text-centered">
            <figure class="image" style="width: 100%; max-width: 80%; margin: 0 auto;">
              <img src="./static/images/qual_3_pavas.png" alt="Sample 3">
            </figure>
            <p><strong>Samples 5&6</strong></p>
          </div>
        </div>

        
      </div>
    </div>
  </section>
  
  

  




<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{hyun2025pavas,
        title={PAVAS: Physics-Aware Video-to-Audio Synthesis},
        author={Hyun-Bin, Oh and Takida, Yuhta and Uesaka, Toshimitsu and Oh, Tae-Hyun and Mitsufuji, Yuki},
        journal={arXiv preprint arXiv:2512.18325},
        year={2025}
      }</code></pre>
  </div>
</section>


<!-- <section class="section" id="Acknowledgment">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgment</h2>
    <p>
      This work was supported by IITP grant funded by Korea government (MSIT) (No.2021-0-02068, Artificial Intelligence Innovation Hub; No.RS-2023-00225630, Development of Artificial Intelligence for Text-based 3D Movie Generation; No.2022-0-00290, Visual Intelligence for Space-Time Understanding and Generation
based on Multi-layered Visual Common Sense; No.2022-0-00124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities).
    </p>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
<!--      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>-->
            <a class="icon-link" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>
              Source code mainly borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>